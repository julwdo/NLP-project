\documentclass{Class/julia}

\usepackage{geometry}
\usepackage{graphicx} % To use \resizebox
\usepackage{array} % For custom column widths
\usepackage{calc} % To use \widthof

\usepackage{siunitx} % Formatting numbers in a table

\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
%\usepackage{placeins}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

\author{Julia Maria Wdowinska}
\date{} % Remove date from the title

\begin{document}

\begin{titlepage}
    \centering
    \vfill
    {\scshape\Large University of Milan \par}
    \vspace{0.5cm}
    {\scshape\large Faculty of Political, Economic and Social Sciences \par}
    \vspace{3cm}
    {\huge
    \textbf{To Bot or Not to Bot?} \\
    \vspace{0.5cm}
    \large Final Project in the \textit{Natural Language Processing} Course \par}
    \vspace{2cm}
    {\large \textbf{Julia Maria Wdowinska} \par}
    \vspace{0.5cm}
    {\large Data Science for Economics \par}
    {\large II year\par}
    {\large Master’s Degree \par}
    {\large Matriculation Number:\ 43288A \par}
\vfill
\begin{center}
\begin{figure}[h!]\centering
 \includegraphics[keepaspectratio=true,scale=0.2]{logo} \\
\end{figure}
\end{center}
\vfill
\begin{center}
{\small{I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.}}
\end{center}
\vfill
    {\large \today \par}
    \vfill
\end{titlepage}

\tableofcontents
%\newpage

\section{Introduction}

Social media platforms such as Instagram, Facebook, or X (formerly Twitter) play a central role in modern~com- munication. X, in particular, has become one of the most influential platforms, gradually replacing traditional media as a major source of news and public discussion. As a result, the content shared on X can shape public~o- pinion and influence social debates.

For this reason, it is important to ensure that accounts are authentic and to limit the presence of automated accounts, commonly known as bots. Bots are automated profiles designed to imitate human activity. While some have useful purposes, many are used to spread misinformation, manipulate public opinion, influence elections,~or promote extremist and discriminatory content.

As bots become more sophisticated, detecting them reliably remains a difficult task. Although research in this area has advanced, relatively few studies apply a multimodal approach that combines both textual and~nu- merical information. In addition, many existing methods rely on a small number of numerical features, often fewer than ten, and do not fully use the textual data available in user profiles and posts.

A recent study by \citet{ArranzEscudero2025} tried to address these limitations. Their work introduced a multimodal framework that combined multiple numerical and categorical profile features with textual information extracted from user descriptions and tweets. The authors also used graph-based data reflecting user~con- nections. This approach achieved a 5.48\% improvement over the previous best result.

This project aims to replicate and critically evaluate this multimodal methodology, looking closely at its~el- ements and trying to improve where possible.

\section{Data Processing}

\subsection{Dataset Overview}

The dataset used in this project was TwiBot-22 \citep{feng2022twibot22}, which is currently the most extensive~X~da- taset available. It is fully labeled (bots vs.\ humans) and contains various X components such as users, tweets, lists, and hashtags. Working with it was challenging due to its size. It includes 1,000,000 users (860,057 human and 139,943 bot accounts) and 88,217,457 tweets, while the previous version, TwiBot-20, contained only 229,580 users, of which only 11,826 were labeled.

To mitigate the computational overhead, only one chunk of tweets (each approx.\ 10–11~GB in size) was used. The analysis was then limited to users present in this chunk, along with their corresponding user data.

\subsection{Feature Engineering}

First, user data was processed to extract features that could help characterize users and distinguish between~human and bot accounts. While \citet{ArranzEscudero2025} introduced 46 features (34 numerical and 12 categorical), in this project 47 numerical and boolean features were created. In addition, as in \citet{ArranzEscudero2025}, the user description was recorded as well. All user-level features are presented in Table~\ref{tab:user_features}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{User-level Features}
\label{tab:user_features}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{listed\_over\_followers, tweets\_over\_followers, listed\_over\_tweets}}
>{\raggedright\arraybackslash}p{\widthof{User description with URLs, mentions, and emails replaced)}}
}
\hline
\textbf{Feature} & \textbf{Description} \\
\hline
name\_length & Number of characters in name \\ \hline
username\_length & Number of characters in username \\ \hline
username\_name\_length\_ratio & Ratio of username length to name length \\ \hline
description & User description with URLs, mentions, and emails replaced by tokens \\ \hline
description\_length & Number of characters in description \\ \hline
has\_name, has\_username, has\_description, has\_url, has\_location, has\_pinned\_tweet & Binary indicators of non-empty fields \\ \hline
has\_bot\_word\_in\_name, has\_bot\_word\_in\_description & Presence of the word ``bot'' in name or description \\ \hline
ratio\_digits\_* & Ratio of digits to total characters in name, username, or description \\ \hline
ratio\_special\_chars\_* & Ratio of special characters (non-alphanumeric) in name, username, or description \\ \hline
name\_upper\_to\_lower\_ratio, username\_upper\_to\_lower\_ratio & Ratio of uppercase to lowercase letters \\ \hline
name\_entropy, username\_entropy & Shannon entropy of name and username \\ \hline
username\_name\_levenshtein & Normalized Levenshtein distance between username and name \\ \hline
description\_sentiment & Compound sentiment score of description (VADER) \\ \hline
cashtag\_in\_description\_count, hashtag\_in\_description\_count, mention\_in\_description\_count, url\_in\_description\_count & Count of respective entities in description \\ \hline
is\_protected, is\_verified & Account protection and verification status \\ \hline
account\_age\_seconds & Account age in seconds \\ \hline
followers\_count, following\_count, listed\_count, tweet\_count & User activity metrics \\ \hline
followers\_over\_following, double\_followers\_over\_following, following\_over\_followers, following\_over\_followers\_squared & Ratios of follower and following counts \\ \hline
following\_over\_total\_connections & Ratio of following count to total connections \\ \hline
listed\_over\_followers, tweets\_over\_followers, listed\_over\_tweets & Ratios between listing, tweet, and follower counts \\ \hline
follower\_rate, following\_rate, listed\_rate, tweet\_rate & Growth rates per account age \\
\hline
\end{tabular}
\end{table}

Next, tweet data was processed to extract features at the tweet level, which were then aggregated at the~user level and merged with the user data. Seven numerical or boolean features were created, and similarly to \citet{ArranzEscudero2025}, concatenated tweet texts were recorded as well. Since tweets in all languages were~con- sidered, only the 10 most recent tweets were used, unlike in \citet{ArranzEscudero2025}, who used 20, because multilingual embedding models accept shorter inputs than English-only models. For the same reason, each tweet text was truncated to 12 tokens. 

A change introduced in this project was the addition of non-truncated tweet texts stored in a list. This was done to test whether concatenating all tweets into one long string and embedding it as a single sentence could distort the information. Both ways of gathering tweet text data were later evaluated in terms of their effect on classifier performance. All tweet-level features created are described in Table~\ref{tab:tweet_features}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Tweet-level Features}
\label{tab:tweet_features}
\begin{threeparttable}
\begin{tabular}{ll}
\hline
\textbf{Feature} & \textbf{Description} \\
\hline
top\_tweets\_truncated\_texts\_concat & Concatenation of tweet texts, each truncated to 12 tokens \\
top\_tweets\_texts & List of tweet texts, no truncation \\
top\_tweets\_reply\_fraction & Fraction of tweets that are replies to another tweet \\
top\_tweets\_num\_distinct\_langs & Number of distinct languages in tweets \\
top\_tweets\_sensitive\_fraction & Fraction of tweets that may contain sensitive content \\
top\_tweets\_avg\_likes & Average number of likes per tweet \\
top\_tweets\_avg\_quotes & Average number of quotes per tweet \\
top\_tweets\_avg\_replies & Average number of replies per tweet \\
top\_tweets\_avg\_retweets & Average number of retweets per tweet \\
\hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item Note:\ All statistics are computed over the 10 most recent tweets per user.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Data Summary}

The final dataset comprised 314,813 users and 57 features:\ 44 numeric, 10 boolean, and 3 text features (two of which contained information on tweet texts). Two columns, top\_tweets\_avg\_quotes and top\_tweets\_avg\_replies, had a large proportion of missing values (56.52\% each). Given the availability of many other features, these~col- umns were removed rather than imputed.

The dataset was highly imbalanced, with 92.46\% of accounts labeled as human and 7.54\% labeled as bots. A stratified 90/10 train-test split was applied to preserve this distribution:

\begin{itemize}
    \item \textbf{Training set:} 283,331 users (90\% of the dataset)  
        \begin{itemize}
            \item Class 0 (human): 261,979 users (92.46\%)  
            \item Class 1 (bot): 21,352 users (7.54\%)
        \end{itemize}
    \item \textbf{Test set:} 31,482 users (10\% of the dataset)  
        \begin{itemize}
            \item Class 0 (human): 29,110 users (92.47\%)  
            \item Class 1 (bot): 2,372 users (7.53\%)
        \end{itemize}
\end{itemize}

\subsection{Feature Selection}

Given the large number of features, some were likely redundant. Therefore, feature selection methods\footnote{To prevent information leakage, feature selection methods as well as subsequent scaling were fitted exclusively on the training set and then applied to the test set, while oversampling was performed exclusively on the training set, ensuring that the test set~re- mained unseen and could provide an unbiased evaluation of model performance.} were~used to identify the most informative ones. These methods were preferred over dimensionality reduction~techniques to keep the features interpretable.

First, pairwise correlations between numeric features were examined (see Figure~\ref{fig:corr_matrix}). Six feature pairs were found to be highly correlated (absolute correlation $>0.8$), as summarized in Table~\ref{tab:high_corr_features}. No features were removed at this stage to avoid arbitrary decisions.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{corr_all_lang.png}
    \caption{Pairwise Correlations}
    \label{fig:corr_matrix}
\end{figure}

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Highly Correlated Feature Pairs}
\label{tab:high_corr_features}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{followers\_over\_following}}
>{\raggedright\arraybackslash}p{\widthof{following\_over\_followers\_squared}}
*{1}{S[table-format=1.2]}
}
\hline
\textbf{Feature 1} & \textbf{Feature 2} & \textbf{Absolute Correlation} \\
\hline
followers\_count & follower\_rate & 0.98 \\
following\_count & following\_rate & 0.98 \\
listed\_count & listed\_rate & 0.99 \\
tweet\_count & tweet\_rate & 0.96 \\
followers\_over\_following & double\_followers\_over\_following & 1.00 \\
following\_over\_followers & following\_over\_followers\_squared & 0.96 \\
\hline
\end{tabular}
\end{table}

Next, the variance of numeric and boolean features was examined. Features with variance below 0.01 were considered to have low discriminatory power and were removed. The removed features and their variances are listed in Table~\ref{tab:low_variance_features}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{Low-Variance Features}
\label{tab:low_variance_features}
\begin{tabular}{lc}
\hline
\textbf{Feature} & \textbf{Variance} \\
\hline
has\_username & 0.000000 \\
listed\_rate & 0.000000 \\
following\_rate & 0.000000 \\
tweet\_rate & 0.000000 \\
follower\_rate & 0.000004 \\
has\_name & 0.000032 \\
ratio\_digits\_in\_description & 0.000829 \\
has\_bot\_word\_in\_name & 0.000924 \\
ratio\_digits\_in\_name & 0.001506 \\
has\_bot\_word\_in\_description & 0.001853 \\
is\_protected & 0.001994 \\
ratio\_special\_chars\_username & 0.002008 \\
top\_tweets\_sensitive\_fraction & 0.004211 \\
\hline
\end{tabular}
\end{table}

Finally, a Gradient Boosting Classifier (GBC) was used to identify the most important numeric and boolean features remaining after variance filtering. The classifier was trained using default hyperparameters:

\begin{itemize}
    \item $n\_estimators = 100$: number of trees
    \item $learning\_rate = 0.1$: step size shrinkage to prevent overfitting
    \item $max\_depth = 3$: maximum depth of each tree
    \item $subsample = 1.0$: fraction of samples used for each tree
    \item $loss = \text{log\_loss}$: loss function minimized during training
    \item $max\_features = \text{None}$: number of features considered per split
\end{itemize}

\noindent These settings specify that 100 trees were built with a maximum depth of 3, all samples were used to build each tree, the log loss function was minimized, and all features were considered at each split. Feature importance~was calculated as the average reduction in the loss function across all trees. Only features with importance above~the median were kept. This approach is more conservative than in \citet{ArranzEscudero2025}, who retained all features with non-zero importance. The selected features and their normalized importance scores are shown in Table~\ref{tab:gbc_features}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{Selected Features and Their Importance}
\label{tab:gbc_features}
\begin{tabular}{lc}
\hline
\textbf{Feature} & \textbf{Importance} \\
\hline
followers\_count & 0.4999 \\
tweet\_count & 0.1094 \\
has\_description & 0.1065 \\
description\_length & 0.0976 \\
following\_count & 0.0624 \\
top\_tweets\_avg\_retweets & 0.0253 \\
is\_verified & 0.0218 \\
account\_age\_seconds & 0.0165 \\
username\_name\_length\_ratio & 0.0094 \\
ratio\_special\_chars\_in\_description & 0.0087 \\
ratio\_special\_chars\_in\_name & 0.0071 \\
hashtag\_in\_description\_count & 0.0056 \\
listed\_over\_followers & 0.0043 \\
listed\_over\_tweets & 0.0039 \\
name\_length & 0.0037 \\
top\_tweets\_num\_distinct\_langs & 0.0032 \\
top\_tweets\_avg\_likes & 0.0023 \\
followers\_over\_following & 0.0021 \\
mention\_in\_description\_count & 0.0016 \\
tweets\_over\_followers & 0.0014 \\
\hline
\end{tabular}
\end{table}

These results are generally consistent with \citet{ArranzEscudero2025}. For example, has\_description was the most important feature in their study, whereas it ranked third here. The feature \texttt{tweet\_count} was the~sec- ond most important in both studies. Small differences are likely due to this analysis being conducted on a~subset of users, while the referenced study considered the full dataset.

\subsection{Scaling}

Following \citet{ArranzEscudero2025}, all numeric features were standardized to have a mean of 0 and a~stand- ard deviation of 1. This step is not needed for tree-based models, but it was justified here because a neural~net- work was used. Scaling the features helps improve both stability and performance for neural networks.

\subsection{Text Embeddings}

After feature selection and scaling, a numerical representation of textual data was created using pre-trained~embedding models. User descriptions were always encoded using \textit{distiluse-base-multilingual-cased-v1}, while tweet texts were encoded using two different models and either as a concatenation or as a list of tweets. This resulted in four embedding versions:

\begin{itemize}
    \item \textbf{v1:} Concatenated tweets encoded using \textit{distiluse-base-multilingual-cased-v1}.
    \item \textbf{v2:} Concatenated tweets encoded using \textit{paraphrase-multilingual-MiniLM-L12-v2}.
    \item \textbf{v3:} Tweets encoded separately using \textit{distiluse-base-multilingual-cased-v1}, followed by mean-pooling\footnote{Mean-pooling means that each tweet in the list was encoded separately and then an average over all tweet embeddings per user was computed.}.
    \item \textbf{v4:} Tweets encoded separately using \textit{paraphrase-multilingual-MiniLM-L12-v2}, followed by mean-pooling.
\end{itemize}

This process was simpler than in \citet{ArranzEscudero2025}, because the embedding models handle~tokenization automatically and there is no need to remove stopwords. In fact, removing stopwords can sometimes reduce performance, since the models rely on the full context to generate embeddings. Another difference is~that the embedding dimension was 512 instead of 768, corresponding to the output size of the multilingual~models used.

\subsection{Class Imbalance Handling}

To address the substantial class imbalance in the dataset, oversampling was performed using the Synthetic~Minor- ity Oversampling Technique (SMOTE) \citep{Chawla2002smote}. Each embedding dimension was treated as an~additional feature during oversampling. After applying SMOTE, the class distribution became:

\begin{itemize}
\item Class 0 (human): 66.67\%
\item Class 1 (bot): 33.33\%
\end{itemize}

This approach slightly improved model performance and worked better than alternative strategies, such as using a weighted loss function (as in \citealp{ArranzEscudero2025}) or applying sampling weights when forming training batches.

The entire data processing pipeline is illustrated in Figure~\ref{fig:data_pipeline}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{diagram1.png}
    \caption{Data Processing Pipeline}
    \label{fig:data_pipeline}
\end{figure}

\section{Model}

In the study by \citet{ArranzEscudero2025}, the best-performing model was a Relational Graph Convolutional Network (RGCN). This type of model uses the graph structure of a social network such as X, combining~information from a user's own features, their connections, the types of these connections, and the features of~neighboring users.

In this project, a standard Graph Convolutional Network (GCN) was explored. Only a single type of edge was considered, representing a ``follows'' relationship between users. Furthermore, only edges connecting users within the selected subset were included. The results differed substantially from those in the original paper.~This is likely due to the artificial nature of the network created by restricting edges to the subset:\ users in the subset may have had connections to users outside it, but these were disregarded, which distorted the graph~structure.

\subsection{Model Architecture}

Consequently, a multi-branch neural network was implemented, following the general architecture of the original paper but without the graph component. The network comprised three branches:\ one for numeric and boolean features, one for description embeddings, and one for tweet embeddings.

Each branch applied a linear transformation, followed by a Leaky ReLU activation and a dropout layer. The outputs of all branches were then concatenated and passed through another linear transformation, followed by a Leaky ReLU activation, a dropout layer, and a final linear transformation producing a one-dimensional output.

\subsection{Model Training and Results}

Several experiments were performed to evaluate and optimize the model.

First, the effect of different embedding variants on model performance was assessed. The neural network~was trained four times, using the same set of hyperparameters for each run, but with different embeddings.

Next, the impact of using only English-language tweets was examined. All steps from feature engineering~to oversampling were repeated, and the neural network was retrained for each of the four embedding variants, with the only difference being the use of English-only embedding models.

Finally, the embedding variant that achieved the highest F1 score, whether all-language or English-only, was selected as the best setting. Hyperparameter optimization was then performed, and the network was retrained using these optimized hyperparameters.

\subsubsection{Effect of Embedding Variants on Model Performance}

Training was performed using mini-batches of 32 samples with a dropout rate of 0.5. The hidden dimension~of the neural network was set to 128. The Binary Cross-Entropy (BCE) loss function, implemented with an~integrated sigmoid layer for numerical stability, was used. The Adam optimizer was applied with a learning rate of $1 \times 10^{-3}$ and a weight decay of $1 \times 10^{-5}$. Predictions were binarized using a threshold of 0.5.

The network was trained for 100 epochs for each embedding variant. The epoch that achieved the highest~F1 score for each variant is reported in Table~\ref{tab:nn_performance_variants}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{Performance Across Embedding Variants}
\label{tab:nn_performance_variants}
\begin{tabular}{lccccc}
\hline
\textbf{Variant} & \textbf{Best F1} & \textbf{Epoch at Best F1} & \textbf{Test Accuracy} & \textbf{Test Precision} & \textbf{Test Recall} \\
\hline
v1 & 0.4314 & 30 & 0.9112 & 0.4166 & 0.4473 \\
v2 & 0.4263 & 8 & 0.9076 & 0.4004 & 0.4557 \\
v3 & 0.4372 & 43 & 0.9069 & 0.4016 & 0.4798 \\
v4 & 0.4330 & 68 & 0.8994 & 0.3764 & 0.5097 \\
\hline
\end{tabular}
\end{table}

The highest F1 score was achieved by the third embedding variant, with 43.72\%. The highest recall was observed for the fourth variant, 50.97\%, which was associated with the second best F1 score of 43.3\%. This~indicates that the approach introduced in this project, where each tweet is embedded separately without truncation and then mean-pooling is applied, slightly outperformed the method of \citet{ArranzEscudero2025}, where~tweet texts were concatenated into a single string before embedding.

\subsubsection{Impact of Using English-Only Tweets on Model Performance}

A question arose as to whether restricting tweets to English only would improve model performance. To~investigate this, the network was trained and evaluated using embeddings derived exclusively from English tweets.

Because English-only embedding models support longer token sequences, the 20 most recent tweets (instead of 10) were used to compute tweet-level statistics. The feature top\_tweets\_num\_distinct\_langs was omitted, since all users had tweets in a single language after filtering. The resulting dataset included 278,227 users and 56 features. As before, the features top\_tweets\_avg\_quotes and top\_tweets\_avg\_replies contained a large proportion of missing values (59.39\% each) and were removed. The class distribution remained imbalanced, with 92.16\% human accounts and 7.84\% bots. A stratified 90/10 train-test split was applied to preserve this ratio:

\begin{itemize}
    \item \textbf{Training set:} 250,404 users (90\% of the dataset)  
        \begin{itemize}
            \item Class 0 (human): 230,765 users (92.16\%)  
            \item Class 1 (bot): 19,639 users (7.84\%)
        \end{itemize}
    \item \textbf{Test set:} 27,823 users (10\% of the dataset)  
        \begin{itemize}
            \item Class 0 (human): 25,641 users (92.16\%)  
            \item Class 1 (bot): 2,182 users (7.84\%)
        \end{itemize}
\end{itemize}

Variance thresholding and feature selection using a Gradient Boosting Classifier (GBC) were performed with the same hyperparameters as before. The selected features and their importance scores are shown in Table~\ref{tab:gbc_features_en}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{Selected Features and Their Importance (English-Only Tweets)}
\label{tab:gbc_features_en}
\begin{tabular}{lc}
\hline
\textbf{Feature} & \textbf{Importance} \\
\hline
followers\_count & 0.4971 \\
description\_length & 0.1402 \\
tweet\_count & 0.1036 \\
following\_count & 0.0678 \\
has\_description & 0.0631 \\
top\_tweets\_avg\_retweets & 0.0303 \\
is\_verified & 0.0230 \\
account\_age\_seconds & 0.0217 \\
username\_name\_length\_ratio & 0.0106 \\
hashtag\_in\_description\_count & 0.0058 \\
name\_length & 0.0055 \\
listed\_over\_followers & 0.0050 \\
double\_followers\_over\_following & 0.0033 \\
listed\_over\_tweets & 0.0030 \\
ratio\_special\_chars\_in\_description & 0.0027 \\
top\_tweets\_avg\_likes & 0.0022 \\
ratio\_special\_chars\_in\_name & 0.0021 \\
tweets\_over\_followers & 0.0019 \\
top\_tweets\_reply\_fraction & 0.0016 \\
\hline
\end{tabular}
\end{table}

Numeric features were standardized to have zero mean and unit variance. User descriptions were embedded~using \textit{all-MiniLM-L6-v2}, while tweets were embedded using the same four approaches as before but with English-only models, yielding the following variants:

\begin{itemize}
    \item \textbf{v1:} Concatenated tweets encoded using \textit{all-MiniLM-L6-v2}.
    \item \textbf{v2:} Concatenated tweets encoded using \textit{paraphrase-MiniLM-L6-v2}.
    \item \textbf{v3:} Tweets encoded separately using \textit{all-MiniLM-L6-v2}, followed by mean-pooling.
    \item \textbf{v4:} Tweets encoded separately using \textit{paraphrase-MiniLM-L6-v2}, followed by mean-pooling.
\end{itemize}

Class imbalance was addressed using SMOTE, resulting in 66.67\% human and 33.33\% bot accounts.

The same neural network hyperparameters as before were used:\ batch size of 32, dropout rate of 0.5, Binary Cross-Entropy loss without class weighting, Adam optimizer with learning rate $1 \times 10^{-3}$ and weight decay~$1 \times 10^{-5}$, and a prediction threshold of 0.5. Each network was trained for 100 epochs, and the best-performing~epoch (by F1 score) for each embedding variant was recorded:

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{8pt}
\caption{Performance Across Embedding Variants (English-Only Tweets)}
\label{tab:nn_performance_variants_en}
\begin{tabular}{lccccc}
\hline
\textbf{Variant} & \textbf{Best F1} & \textbf{Epoch at Best F1} & \textbf{Test Accuracy} & \textbf{Test Precision} & \textbf{Test Recall} \\
\hline
v1 & 0.4125 & 6  & 0.8920 & 0.3597 & 0.4835 \\
v2 & 0.4052 & 2  & 0.8902 & 0.3522 & 0.4771 \\
v3 & 0.4149 & 7  & 0.8917 & 0.3601 & 0.4895 \\
v4 & 0.4187 & 62 & 0.8988 & 0.3811 & 0.4647 \\
\hline
\end{tabular}
\end{table}

The highest F1 score (41.87\%) was achieved with embedding variant \textit{v4}. The highest recall (48.95\%) occurred with \textit{v3}, which also produced the second highest F1 score (41.49\%). As previously, embedding tweets separately and then performing mean pooling yielded better results. Overall, however, F1 scores were slightly lower than those obtained using tweets in all languages. This may be due to the smaller dataset size for English-only~tweets (278,227 versus 314,813 users). Nevertheless, these results suggest that filtering tweets by language is not strictly necessary for bot detection, allowing all tweets to be used and reducing preprocessing time.

\subsubsection{Final Model Selection and Evaluation}

The embedding variant that achieved the best performance in terms of F1 score was the all-languages variant,~\textit{v3}, and it was selected for final model tuning.

Instead of using a separate training, validation, and test split as in \citet{ArranzEscudero2025}, a 5-fold cross-validation\footnote{Cross-validation provides a more robust estimate of model performance by reducing the variance associated with a single~validation split.} was performed on the training set. In each of the five iterations, 80\% of the training data was used for training and the remaining 20\% for validation. The parameter grid for tuning was as follows:

\begin{verbatim}
param_grid = {
    "batch_size": [16, 32, 64],
    "hidden_dim": [64, 128, 256],
    "dropout": [0.3, 0.5, 0.7],
    "lr": [1e-2, 1e-3, 5e-4],
    "weight_decay": [0, 1e-5, 1e-4]
}
\end{verbatim}

To reduce computation time, only 10 random combinations out of 243 were tested. For each combination~and each iteration, the network was trained for 50 epochs with early stopping if the validation F1 did not improve for 3 consecutive epochs. The highest F1 score per iteration was recorded, and the mean of these scores~across iterations was computed. This mean F1 score was used to compare different parameter combinations. The~re- sults are summarized in Table~\ref{tab:cv_results}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Cross-Validation Results}
\label{tab:cv_results}
\begin{tabular}{cccccc}
\hline
batch\_size & hidden\_dim & dropout & lr & weight\_decay & mean\_best\_f1 \\
\hline
32 & 64  & 0.5 & 0.0005 & 0.0001 & 0.7960 \\
32 & 256 & 0.5 & 0.01   & 0.0001 & 0.6992 \\
64 & 256 & 0.5 & 0.0005 & 0      & 0.8742 \\
64 & 64  & 0.3 & 0.001  & 0.0001 & 0.8082 \\
16 & 256 & 0.5 & 0.01   & 0.0005 & 0.6894 \\
64 & 256 & 0.7 & 0.01   & 0      & 0.7182 \\
64 & 64  & 0.3 & 0.01   & 0.0001 & 0.7254 \\
64 & 128 & 0.5 & 0.001  & 0      & 0.8407 \\
16 & 128 & 0.5 & 0.001  & 0.0001 & 0.7973 \\
64 & 128 & 0.5 & 0.001  & 0.0001 & 0.8252 \\
\hline
\end{tabular}
\end{table}

The highest mean F1 score (87.42\%) was achieved with batch size 64, hidden dimension 256, dropout 0.5, learning rate 0.0005, and weight decay 0. After selecting this combination, the network was retrained on the~en- tire training set for 100 epochs and evaluated on the test set. The best performing epoch in terms of F1 was:

\begin{itemize}
    \item Best F1: 0.4310
    \item Epoch at best F1: 15
    \item Test Accuracy: 0.9136
    \item Test Precision: 0.4277
    \item Test Recall: 0.4342
\end{itemize}

Surprisingly, this F1 score was slightly lower than the one achieved with the previous parameters without~hyperparameter tuning.

\section{Conclusions}

In this project, a multimodal approach to X bot detection was implemented and evaluated using a subset of the TwiBot-22 dataset \citep{feng2022twibot22}. A large number of numerical and boolean user features were carefully~curated and enriched with additional features extracted from users’ most recent tweets. The most informative features were selected through variance thresholding and Gradient Boosting Classifier-based selection, and numerical features were scaled. Textual data, including user descriptions and tweet texts, was embedded using pre-trained models. Tweets were represented in two ways, either as truncated tweets concatenated into a string or as non-truncated tweets gathered in a list. Combined with two different models for tweets and a single model for descriptions, this produced four embedding variants. Significant class imbalance, approximately 7\% bots, was addressed using SMOTE.

Regarding the model, a Graph Convolutional Network was initially explored but did not yield satisfactory results. A multibranch neural network without the graph component was therefore preferred. The impact~of different embedding variants on performance was evaluated, and variant \textit{v3} achieved the best F1, 43.72\%,~followed by variant \textit{v4} with 43.3\%. This showed that encoding tweets separately and then applying mean pooling produces better results than concatenating tweets into a single string, as in \citet{ArranzEscudero2025}.~The effect of using only English-language tweets was also assessed. After filtering for English-only~tweets and repeating all processing steps, results were slightly worse than those obtained with all-language tweets, with the best F1 at 41.87\%, but variants \textit{v3} and \textit{v4} remained superior to variants \textit{v1} and \textit{v2}. Finally, the best embedding variant, all languages variant \textit{v3}, was selected, hyperparameters were fine-tuned via 5-fold cross-validation, and the network was retrained on the full training set. The results were slightly lower than those obtained using~the initially chosen parameters, which was somewhat surprising.

An important observation is the large discrepancy between F1 scores on validation sets (68.94\%–87.42\%) during cross-validation and those on the test set (40.52\%–43.72\%). Validation scores were close to or exceeded the results reported by \citet{ArranzEscudero2025}, whereas test scores were considerably lower. This suggests that the results in \citet{ArranzEscudero2025} may be overly optimistic and potentially influenced by information leakage, as their processing steps are applied before splitting the data. In contrast, in this study, the data was first split into training and test sets, and all processing steps were fitted on the training set and then applied to the test set, ensuring that the test set remained independent and unbiased.

\section{Limitations and Future Work}

Despite the careful design of this study, several limitations should be noted.

Only a subset of the TwiBot-22 dataset was used, which restricted the network structure and may have~limited the model’s ability to leverage relational information. Consequently, a multibranch neural network was~used instead of a graph-based model, which may have reduced performance by ignoring connections between users.

Feature and text representation choices also imposed limitations. Feature selection, while effective, may~have overlooked interactions between features that are weak individually but informative in combination. Text~embeddings involved truncation or mean pooling, potentially losing information from users with many tweets.~More- over, the choice of embedding models may have limited the richness of textual representations.

Finally, class imbalance and evaluation constraints should be considered. Although SMOTE mitigated~imbalance, synthetic oversampling may not fully capture the true distribution of bot behavior. Discrepancies between cross-validation and test set performance indicate some sensitivity to data splits, and no external~datasets were used to assess generalization.

These limitations highlight opportunities for future work, such as using the full TwiBot-22 dataset, exploring graph-based architectures, incorporating more sophisticated feature interactions, experimenting with alternative embedding strategies, and evaluating the approach on additional datasets to improve generalization and~robust- ness.

\section*{AI Usage Disclaimer}

This work was assisted by AI tools (mainly OpenAI’s GPT-5, both free and paid versions) for research~guidance, including understanding concepts such as class imbalance handling and graph neural networks, for refining and drafting code to process data or display statistics, and for improving language and clarity in the final report.~The AI was used solely as a support tool; all research decisions, analyses, interpretations, and conclusions are my~own. I remain fully responsible for the content, accuracy, and claims presented in this project.

\printbibliography{bib}
\end{document}