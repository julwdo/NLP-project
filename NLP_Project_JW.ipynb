{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fV-2Mc6kjhr3GdesgLaaXJLp6qV5bBC9",
      "authorship_tag": "ABX9TyMzSBKxiRD8+f846z2pr13c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julwdo/NLP-project/blob/main/NLP_Project_JW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get install openjdk-17-jdk-headless -qq > /dev/null # OpenJDK 17\n",
        "#!wget --show-progress https://dlcdn.apache.org/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz # Apache Spark 3.5.6 with Hadoop 3 support\n",
        "#!tar xf spark-3.5.6-bin-hadoop3.tgz\n",
        "#!pip install -q findspark\n",
        "#!pip install -q spark-nlp==6.1.2\n",
        "#!pip install -q xgboost==3.0.4\n",
        "#!pip install -q --upgrade pyspark==3.5.6"
      ],
      "metadata": {
        "id": "mQ8T-LAUzBeh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.5.6-bin-hadoop3'"
      ],
      "metadata": {
        "id": "WHawKcY3TCNK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"BotDetection\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2\")\n",
        "    .config(\"spark.driver.memory\", \"8g\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hBPmK_AXjToH",
        "outputId": "5044ec0e-7f78-42f2-f699-f84254ab16d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a4ab31f5730>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2a77d8fc53d9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.6</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BotDetection</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import math\n",
        "from collections import Counter\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import StringType, NumericType, BooleanType\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import XlmRoBertaSentenceEmbeddings\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from xgboost.spark import SparkXGBClassifier"
      ],
      "metadata": {
        "id": "J-C_Oc4BzG2s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "1dvQfYXH_WoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec77297c-51ca-4d25-f3df-fb76183c8026"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "nXzBU7MKS-dT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!gcloud init"
      ],
      "metadata": {
        "id": "x3jli8AUVuOm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"twibot-22\"\n",
        "file_names = [\"user.jsonl\", \"label.csv\", \"tweet_0.jsonl\"]\n",
        "\n",
        "for file_name in file_names:\n",
        "    local_path = f\"/content/{file_name}\"\n",
        "    if not os.path.exists(local_path):\n",
        "        !gsutil cp gs://{bucket_name}/{file_name} {local_path}\n",
        "    else:\n",
        "        print(f\"{file_name} already exists locally, skipping download.\")"
      ],
      "metadata": {
        "id": "T20eeOGhZdsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30027c0b-13c2-4d99-f27c-045b573174fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user.jsonl already exists locally, skipping download.\n",
            "label.csv already exists locally, skipping download.\n",
            "tweet_0.jsonl already exists locally, skipping download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users = spark.read.json(f\"/content/user.jsonl\")\n",
        "#users.printSchema()"
      ],
      "metadata": {
        "id": "4ljK4HtA_MmV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#users.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "zfqG1CcW_Q1c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_selected = users.select(\n",
        "    F.col(\"id\"),\n",
        "    F.col(\"name\"),\n",
        "    F.col(\"username\"),\n",
        "    F.col(\"created_at\"),\n",
        "    F.col(\"description\"),\n",
        "    F.col(\"url\"),\n",
        "    F.col(\"entities.description.cashtags\"),\n",
        "    F.col(\"entities.description.hashtags\"),\n",
        "    F.col(\"entities.description.mentions\"),\n",
        "    F.col(\"entities.description.urls\"),\n",
        "    F.col(\"location\"),\n",
        "    F.col(\"pinned_tweet_id\"),\n",
        "    F.col(\"profile_image_url\"),\n",
        "    F.col(\"protected\"),\n",
        "    F.col(\"public_metrics.followers_count\"),\n",
        "    F.col(\"public_metrics.following_count\"),\n",
        "    F.col(\"public_metrics.listed_count\"),\n",
        "    F.col(\"public_metrics.tweet_count\"),\n",
        "    F.col(\"verified\")\n",
        "    )"
      ],
      "metadata": {
        "id": "GQAYasduAYqH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#users_selected.printSchema()"
      ],
      "metadata": {
        "id": "Ce5ND56SEn8R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#users_selected.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "m1nb1FybEtUp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = spark.read.csv(f\"/content/label.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "sDJSz4DuE6_2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S3i6OraGnzV",
        "outputId": "a11b5aff-0bf0-4e35-d20b-01f5429c3397"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|id                  |label|\n",
            "+--------------------+-----+\n",
            "|u1217628182611927040|human|\n",
            "|u2664730894         |human|\n",
            "|u1266703520205549568|human|\n",
            "|u1089159225148882949|human|\n",
            "|u36741729           |bot  |\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_labeled = users_selected.join(labels, users_selected.id == labels.id, \"left\").drop(labels.id)"
      ],
      "metadata": {
        "id": "Dx7pGZ1rGpSJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#users_labeled.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "yxJV0qKaG_Et"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Summary of missing values:')\n",
        "#users_labeled.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in users_labeled.columns]).show()"
      ],
      "metadata": {
        "id": "e29uv2D-P3Zz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sentiment(text):\n",
        "  return sia.polarity_scores(text)[\"compound\"]\n",
        "\n",
        "vader_udf = F.udf(vader_sentiment, FloatType())"
      ],
      "metadata": {
        "id": "hR9nb9Q6P7CJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shannon_entropy(string):\n",
        "    if string.strip() == \"\":\n",
        "        return 0.0\n",
        "    counts = Counter(string)\n",
        "    length = len(string)\n",
        "    return -sum((count/length) * math.log2(count/length) for count in counts.values())\n",
        "\n",
        "entropy_udf = F.udf(shannon_entropy, FloatType())"
      ],
      "metadata": {
        "id": "T9Gc6ww0beC5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = F.current_timestamp()"
      ],
      "metadata": {
        "id": "3JkuBNxKOLqc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_features = users_labeled.select(\n",
        "    F.col(\"id\"),\n",
        "    F.length(F.col(\"name\")).alias(\"name_length\"),\n",
        "    F.length(F.col(\"username\")).alias(\"username_length\"),\n",
        "    (F.length(F.col(\"username\")) / F.greatest(F.length(F.col(\"name\")), F.lit(1))).alias(\"username_name_length_ratio\"),\n",
        "    F.regexp_replace(F.regexp_replace(F.regexp_replace(F.col(\"description\"), r\"https?://t\\.co/\\S+\", \"<URL>\"), r\"(?<=^|\\s)@\\w+\", \"<USER>\"), r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\").alias(\"description\"),\n",
        "    F.length(F.col(\"description\")).alias(\"description_length\"),\n",
        "    F.when(F.col(\"name\") == \"\", False).otherwise(True).alias(\"has_name\"),\n",
        "    F.when(F.col(\"username\") == \"\", False).otherwise(True).alias(\"has_username\"),\n",
        "    F.when(F.col(\"description\") == \"\", False).otherwise(True).alias(\"has_description\"),\n",
        "    F.when(F.col(\"url\") == \"\", False).otherwise(True).alias(\"has_url\"),\n",
        "    F.when(F.col(\"location\").isNull() | (F.col(\"location\") == \"\"), False).otherwise(True).alias(\"has_location\"),\n",
        "    F.when(F.col(\"pinned_tweet_id\").isNull(), False).otherwise(True).alias(\"has_pinned_tweet\"),\n",
        "    F.col(\"name\").rlike(\"(?i)\\\\bbot\\\\b\").alias(\"has_bot_word_in_name\"),\n",
        "    F.col(\"description\").rlike(\"(?i)\\\\bbot\\\\b\").alias(\"has_bot_word_in_description\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"name\"), \"[^\\\\d]\", \"\")) / F.greatest(F.length(F.col(\"name\")), F.lit(1))).alias(\"ratio_digits_in_name\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"username\"), \"[^\\\\d]\", \"\")) / F.greatest(F.length(F.col(\"username\")), F.lit(1))).alias(\"ratio_digits_in_username\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"description\"), \"[^\\\\d]\", \"\")) / F.greatest(F.length(F.col(\"description\")), F.lit(1))).alias(\"ratio_digits_in_description\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"name\"), \"[A-Za-z0-9 ]\", \"\")) / F.greatest(F.length(F.col(\"name\")), F.lit(1))).alias(\"ratio_special_chars_in_name\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"username\"), \"[A-Za-z0-9 ]\", \"\")) / F.greatest(F.length(F.col(\"username\")), F.lit(1))).alias(\"ratio_special_chars_in_username\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"description\"), \"[A-Za-z0-9 ]\", \"\")) / F.greatest(F.length(F.col(\"description\")), F.lit(1))).alias(\"ratio_special_chars_in_description\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"name\"), \"[^A-Z]\", \"\")) / F.greatest(F.length(F.regexp_replace(F.col(\"name\"), \"[^a-z]\", \"\")), F.lit(1))).alias(\"name_upper_to_lower_ratio\"),\n",
        "    (F.length(F.regexp_replace(F.col(\"username\"), \"[^A-Z]\", \"\")) / F.greatest(F.length(F.regexp_replace(F.col(\"username\"), \"[^a-z]\", \"\")), F.lit(1))).alias(\"username_upper_to_lower_ratio\"),\n",
        "    entropy_udf(F.col(\"name\")).alias(\"name_entropy\"),\n",
        "    entropy_udf(F.col(\"username\")).alias(\"username_entropy\"),\n",
        "    (F.levenshtein(F.col(\"username\"), F.col(\"name\")) / F.greatest(F.length(F.col(\"username\")), F.length(F.col(\"name\")), F.lit(1))).alias(\"username_name_levenshtein\"),\n",
        "    vader_udf(F.col(\"description\")).alias(\"description_sentiment\"),\n",
        "    F.when(F.col(\"cashtags\").isNotNull(), F.size(F.col(\"cashtags\"))).otherwise(F.lit(0)).alias(\"cashtag_in_description_count\"),\n",
        "    F.when(F.col(\"hashtags\").isNotNull(), F.size(F.col(\"hashtags\"))).otherwise(F.lit(0)).alias(\"hashtag_in_description_count\"),\n",
        "    F.when(F.col(\"mentions\").isNotNull(), F.size(F.col(\"mentions\"))).otherwise(F.lit(0)).alias(\"mention_in_description_count\"),\n",
        "    F.when(F.col(\"urls\").isNotNull(), F.size(F.col(\"urls\"))).otherwise(F.lit(0)).alias(\"url_in_description_count\"),\n",
        "    F.col(\"protected\").alias(\"is_protected\"),\n",
        "    F.col(\"verified\").alias(\"is_verified\"),\n",
        "    (F.unix_timestamp(now) - F.unix_timestamp(F.to_timestamp(\"created_at\"))).alias(\"account_age_seconds\"),\n",
        "    F.col(\"followers_count\"),\n",
        "    F.col(\"following_count\"),\n",
        "    F.col(\"listed_count\"),\n",
        "    F.col(\"tweet_count\"),\n",
        "    (F.col(\"followers_count\") / F.greatest(F.col(\"following_count\"), F.lit(1))).alias(\"followers_over_following\"),\n",
        "    (2 * F.col(\"followers_count\") / F.greatest(F.col(\"following_count\"), F.lit(1))).alias(\"double_followers_over_following\"),\n",
        "    (F.col(\"following_count\") / F.greatest(F.col(\"followers_count\"), F.lit(1))).alias(\"following_over_followers\"),\n",
        "    (F.col(\"following_count\") / F.greatest(F.col(\"followers_count\") ** 2, F.lit(1))).alias(\"following_over_followers_squared\"),\n",
        "    (F.col(\"following_count\") / F.greatest(F.col(\"followers_count\") + F.col(\"following_count\"), F.lit(1))).alias(\"following_over_total_connections\"),\n",
        "    (F.col(\"listed_count\") / F.greatest(F.col(\"followers_count\"), F.lit(1))).alias(\"listed_over_followers\"),\n",
        "    (F.col(\"tweet_count\") / F.greatest(F.col(\"followers_count\"), F.lit(1))).alias(\"tweets_over_followers\"),\n",
        "    (F.col(\"listed_count\") / F.greatest(F.col(\"tweet_count\"), F.lit(1))).alias(\"listed_over_tweets\"),\n",
        "    (F.col(\"followers_count\") / (F.unix_timestamp(now) - F.unix_timestamp(F.to_timestamp(\"created_at\")))).alias(\"follower_rate\"),\n",
        "    (F.col(\"following_count\") / (F.unix_timestamp(now) - F.unix_timestamp(F.to_timestamp(\"created_at\")))).alias(\"following_rate\"),\n",
        "    (F.col(\"listed_count\") / (F.unix_timestamp(now) - F.unix_timestamp(F.to_timestamp(\"created_at\")))).alias(\"listed_rate\"),\n",
        "    (F.col(\"tweet_count\") / (F.unix_timestamp(now) - F.unix_timestamp(F.to_timestamp(\"created_at\")))).alias(\"tweet_rate\"),\n",
        "    F.col(\"label\")\n",
        "    )"
      ],
      "metadata": {
        "id": "S2TklOE1NR4W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#user_features.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "cNzg48OdN_Hk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = spark.read.json(f\"/content/tweet_0.jsonl\")\n",
        "#tweets.printSchema()"
      ],
      "metadata": {
        "id": "BoXGa7V54LOz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "WQcJouz95AqR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_selected = tweets.select(\n",
        "    F.col(\"id\"),\n",
        "    F.regexp_replace(F.regexp_replace(F.regexp_replace(F.col(\"text\"), r\"https?://t\\.co/\\S+\", \"<URL>\"), r\"(?<=^|\\s)@\\w+\", \"<USER>\"), r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"<EMAIL>\").alias(\"text\"),\n",
        "    F.concat(F.lit(\"u\"), F.col(\"author_id\")).alias(\"author_id\"),\n",
        "    F.col(\"created_at\"),\n",
        "    F.when(F.col(\"in_reply_to_user_id\").isNull(), False).otherwise(True).cast(\"int\").alias(\"is_reply\"),\n",
        "    F.col(\"lang\"),\n",
        "    F.col(\"possibly_sensitive\").cast(\"int\").alias(\"is_sensitive\"),\n",
        "    F.col(\"public_metrics.like_count\"),\n",
        "    F.col(\"public_metrics.quote_count\"),\n",
        "    F.col(\"public_metrics.reply_count\"),\n",
        "    F.col(\"public_metrics.retweet_count\")\n",
        "    )"
      ],
      "metadata": {
        "id": "YUIgowMCnixE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets_selected.printSchema()"
      ],
      "metadata": {
        "id": "Ksw3FbI0oI30"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets_selected.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "Olt6LrVdNQEB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Summary of missing values:')\n",
        "#tweets_selected.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in tweets_selected.columns]).show()"
      ],
      "metadata": {
        "id": "nwL2Tp8gNydF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate each tweet to 20 tokens\n",
        "def truncate_text(text, max_tokens=20):\n",
        "    tokens = text.split()\n",
        "    return \" \".join(tokens[:max_tokens])\n",
        "\n",
        "truncate_udf = F.udf(lambda x: truncate_text(x, 20), StringType())\n",
        "\n",
        "tweets_truncated = tweets_selected.withColumn(\n",
        "    \"text_truncated\", truncate_udf(\"text\")\n",
        ")"
      ],
      "metadata": {
        "id": "U99XIObrSVHW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets_truncated.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "wY8BAFM5Utlb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter top tweets per author\n",
        "top_n = 10\n",
        "\n",
        "window = Window.partitionBy(\"author_id\").orderBy(F.col(\"created_at\").desc())\n",
        "tweets_truncated = tweets_truncated.withColumn(\"rank\", F.row_number().over(window))\n",
        "tweets_filtered = tweets_truncated.filter(F.col(\"rank\") <= top_n).drop(\"rank\")"
      ],
      "metadata": {
        "id": "3jkN_YbEKD1n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = \"text\"\n",
        "\n",
        "tweet_features = tweets_filtered.groupBy(\"author_id\").agg(\n",
        "    F.concat_ws(\" \", F.collect_list(text_column)).alias(\"tweets_last20_concatenated_text\"),\n",
        "    F.avg(F.col(\"is_reply\")).alias(\"tweets_last20_reply_fraction\"),\n",
        "    F.countDistinct(\"lang\").alias(\"tweets_last20_num_distinct_langs\"),\n",
        "    F.avg(F.col(\"is_sensitive\")).alias(\"tweets_last20_sensitive_fraction\"),\n",
        "    F.avg(F.col(\"like_count\")).alias(\"tweets_last20_avg_likes\"),\n",
        "    F.avg(F.col(\"quote_count\")).alias(\"tweets_last20_avg_quotes\"),\n",
        "    F.avg(F.col(\"reply_count\")).alias(\"tweets_last20_avg_replies\"),\n",
        "    F.avg(F.col(\"retweet_count\")).alias(\"tweets_last20_avg_retweets\")\n",
        "    )"
      ],
      "metadata": {
        "id": "dOdcif7GIdfN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join user features with aggregated tweet features\n",
        "enriched_user_features = user_features.join(\n",
        "    tweet_features,\n",
        "    user_features.id == tweet_features.author_id,\n",
        "    how=\"inner\"\n",
        ").drop(\"author_id\")"
      ],
      "metadata": {
        "id": "ezqCAy0aPDmS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enriched_user_features.printSchema()"
      ],
      "metadata": {
        "id": "tw__NrC-khyD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#enriched_user_features.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "Z4pZW8hmS9cZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the fraction of data to use (e.g., 0.5 for 50%)\n",
        "DATA_FRACTION = 0.01\n",
        "\n",
        "assert 0 < DATA_FRACTION <= 1, 'ERROR: DATA_FRACTION must be between 0 and 1 (exclusive).'"
      ],
      "metadata": {
        "id": "IdvPet3jt14x"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample the dataset according to DATA_FRACTION\n",
        "if DATA_FRACTION < 1:\n",
        "  print(f'Sampling {DATA_FRACTION * 100:.0f}% of the dataset.')\n",
        "  enriched_user_features = enriched_user_features.sample(False, DATA_FRACTION, 42)\n",
        "else:\n",
        "  print('Using the entire dataset.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHf1GiQhoxhn",
        "outputId": "11e71f38-072b-4079-c691-0bc563dc827f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling 1% of the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = enriched_user_features.select(\n",
        "    F.col(\"id\"),\n",
        "    F.col(\"description\"),\n",
        "    F.col(\"tweets_last20_concatenated_text\")\n",
        "    )"
      ],
      "metadata": {
        "id": "aJOcmgbKym9c"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desc_doc = DocumentAssembler() \\\n",
        "    .setInputCol(\"description\") \\\n",
        "    .setOutputCol(\"description_doc\")\n",
        "\n",
        "desc_embed = XlmRoBertaSentenceEmbeddings.pretrained(\"sent_xlm_roberta_base\", \"xx\") \\\n",
        "    .setInputCols([\"description_doc\"]) \\\n",
        "    .setOutputCol(\"description_embeddings\")\n",
        "\n",
        "tweets_doc = DocumentAssembler() \\\n",
        "    .setInputCol(\"tweets_last20_concatenated_text\") \\\n",
        "    .setOutputCol(\"tweets_doc\")\n",
        "\n",
        "tweets_embed = XlmRoBertaSentenceEmbeddings.pretrained(\"sent_xlm_roberta_base\", \"xx\") \\\n",
        "    .setInputCols([\"tweets_doc\"]) \\\n",
        "    .setOutputCol(\"tweets_embeddings\")\n",
        "\n",
        "pipeline = Pipeline(stages=[desc_doc, desc_embed, tweets_doc, tweets_embed])\n",
        "\n",
        "model = pipeline.fit(text_features)\n",
        "text_embeddings = model.transform(text_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUWF7g2e0bO",
        "outputId": "6400e2fc-040e-4826-f69a-97eae8c48171"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent_xlm_roberta_base download started this may take some time.\n",
            "Approximate size to download 619.5 MB\n",
            "[OK!]\n",
            "sent_xlm_roberta_base download started this may take some time.\n",
            "Approximate size to download 619.5 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text_embeddings.printSchema()"
      ],
      "metadata": {
        "id": "gKsAYwY1geco"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_module = text_embeddings.select(\n",
        "    \"id\",\n",
        "    F.col(\"description_embeddings\")[0][\"embeddings\"].alias(\"description_vector\"),\n",
        "    F.col(\"tweets_embeddings\")[0][\"embeddings\"].alias(\"tweets_vector\")\n",
        "    )"
      ],
      "metadata": {
        "id": "yo_KpfDGrRm6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text_module.printSchema()"
      ],
      "metadata": {
        "id": "ry6Y5iLNzWzZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_module = enriched_user_features.drop(\n",
        "    \"description\",\n",
        "    \"tweets_last20_concatenated_text\"\n",
        "    )"
      ],
      "metadata": {
        "id": "wjt0Pw8PDpSe"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#user_module.printSchema()"
      ],
      "metadata": {
        "id": "HBf2D6ATEBJl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation, test split\n",
        "train_frac = 0.7\n",
        "val_frac = 0.2\n",
        "test_frac = 0.1\n",
        "\n",
        "train_df, val_df, test_df = user_module.randomSplit([train_frac, val_frac, test_frac], seed=42)"
      ],
      "metadata": {
        "id": "YzSLM2tk3R_Z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numerical features\n",
        "numeric_cols = [\n",
        "    f.name for f in train_df.schema.fields if isinstance(f.dataType, NumericType)\n",
        "    ]\n",
        "\n",
        "stats_df = train_df.select([\n",
        "    F.mean(c).alias(f\"{c}_mean\") for c in numeric_cols\n",
        "    ] + [\n",
        "        F.stddev(c).alias(f\"{c}_std\") for c in numeric_cols\n",
        "        ])\n",
        "\n",
        "# train_df\n",
        "scaled_train_df = train_df.crossJoin(stats_df)\n",
        "\n",
        "for c in numeric_cols:\n",
        "    scaled_train_df = scaled_train_df.withColumn(f\"{c}_scaled\",\n",
        "     (F.col(c) - F.col(f\"{c}_mean\")) / F.col(f\"{c}_std\"))\n",
        "\n",
        "# val_df\n",
        "scaled_val_df = val_df.crossJoin(stats_df)\n",
        "\n",
        "for c in numeric_cols:\n",
        "    scaled_val_df = scaled_val_df.withColumn(f\"{c}_scaled\",\n",
        "     (F.col(c) - F.col(f\"{c}_mean\")) / F.col(f\"{c}_std\"))\n",
        "\n",
        "# test_df\n",
        "scaled_test_df = test_df.crossJoin(stats_df)\n",
        "\n",
        "for c in numeric_cols:\n",
        "    scaled_test_df = scaled_test_df.withColumn(f\"{c}_scaled\",\n",
        "     (F.col(c) - F.col(f\"{c}_mean\")) / F.col(f\"{c}_std\"))\n",
        "\n",
        "# Select relevant cols\n",
        "cols = [\n",
        "    f.name for f in scaled_train_df.schema.fields\n",
        "    if f.name not in numeric_cols\n",
        "    and not f.name.endswith(\"_mean\")\n",
        "    and not f.name.endswith(\"_std\")\n",
        "]\n",
        "\n",
        "scaled_train_df = scaled_train_df.select(cols)\n",
        "scaled_val_df = scaled_val_df.select(cols)\n",
        "scaled_test_df = scaled_test_df.select(cols)"
      ],
      "metadata": {
        "id": "yexhFmB2TEcp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaled_train_df.show(5)"
      ],
      "metadata": {
        "id": "Q2FK7kGxh4tD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bool_cols = [\n",
        "    f.name for f in scaled_train_df.schema.fields if isinstance(f.dataType, BooleanType)\n",
        "    ]\n",
        "\n",
        "select_exprs = [\n",
        "    # Cast all boolean columns to double\n",
        "    F.col(c).cast(\"double\").alias(c) for c in bool_cols\n",
        "] + [\n",
        "    # Cast label column: 1 if \"bot\", 0 if \"human\"\n",
        "    F.when(F.col(\"label\") == \"bot\", 1.0).otherwise(0.0).alias(\"label\")\n",
        "] + [\n",
        "    F.col(c) for c in scaled_train_df.columns if c not in bool_cols + [\"label\"]\n",
        "]\n",
        "\n",
        "prepared_train_df = scaled_train_df.select(*select_exprs)\n",
        "prepared_val_df = scaled_val_df.select(*select_exprs)\n",
        "prepared_test_df = scaled_test_df.select(*select_exprs)"
      ],
      "metadata": {
        "id": "JxID6aaS2K67"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_train_df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ALvnhbxg1K6",
        "outputId": "c42b1729-3533-4dd8-9417-a5bdc2601a23"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[has_name: double, has_username: double, has_description: double, has_url: double, has_location: double, has_pinned_tweet: double, has_bot_word_in_name: double, has_bot_word_in_description: double, is_protected: double, is_verified: double, label: double, id: string, name_length_scaled: double, username_length_scaled: double, username_name_length_ratio_scaled: double, description_length_scaled: double, ratio_digits_in_name_scaled: double, ratio_digits_in_username_scaled: double, ratio_digits_in_description_scaled: double, ratio_special_chars_in_name_scaled: double, ratio_special_chars_in_username_scaled: double, ratio_special_chars_in_description_scaled: double, name_upper_to_lower_ratio_scaled: double, username_upper_to_lower_ratio_scaled: double, name_entropy_scaled: double, username_entropy_scaled: double, username_name_levenshtein_scaled: double, description_sentiment_scaled: double, cashtag_in_description_count_scaled: double, hashtag_in_description_count_scaled: double, mention_in_description_count_scaled: double, url_in_description_count_scaled: double, account_age_seconds_scaled: double, followers_count_scaled: double, following_count_scaled: double, listed_count_scaled: double, tweet_count_scaled: double, followers_over_following_scaled: double, double_followers_over_following_scaled: double, following_over_followers_scaled: double, following_over_followers_squared_scaled: double, following_over_total_connections_scaled: double, listed_over_followers_scaled: double, tweets_over_followers_scaled: double, listed_over_tweets_scaled: double, follower_rate_scaled: double, following_rate_scaled: double, listed_rate_scaled: double, tweet_rate_scaled: double, tweets_last20_reply_fraction_scaled: double, tweets_last20_num_distinct_langs_scaled: double, tweets_last20_sensitive_fraction_scaled: double, tweets_last20_avg_likes_scaled: double, tweets_last20_avg_quotes_scaled: double, tweets_last20_avg_replies_scaled: double, tweets_last20_avg_retweets_scaled: double]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_df = prepared_train_df.limit(10)  # only 10 rows"
      ],
      "metadata": {
        "id": "KmGptAdzpLpl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Train XGBoost to obtain feature importance\n",
        "feature_cols = [\n",
        "    f.name for f in prepared_train_df.schema.fields\n",
        "    if f.name in bool_cols\n",
        "    or f.name.endswith(\"_scaled\")\n",
        "    ][:3]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"INFO\")\n",
        "\n",
        "xgb = SparkXGBClassifier(\n",
        "    num_workers=spark.sparkContext.defaultParallelism,\n",
        "    missing=0.0,\n",
        "    max_depth=2,\n",
        "    n_estimators=2,\n",
        "    eval_metric=\"logloss\",\n",
        "    verbosity=1\n",
        ")\n",
        "\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "  .addGrid(xgb.max_depth, [2])\\\n",
        "  .addGrid(xgb.n_estimators, [2])\\\n",
        "  .build()\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "cv = CrossValidator(estimator=xgb, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=2)\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, cv])\n",
        "\n",
        "pipelineModel = pipeline.fit(small_train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "P5V9EHzhdi5y",
        "outputId": "a8678e2b-6190-4640-826d-c04c043b3e7a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1355211769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mpipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.5.6-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.6-bin-hadoop3/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.6-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.5.6-bin-hadoop3/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjYR2QH7pOcj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}